{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets build a simple dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train contains three features for each example:<br>\n",
    "Brown Color (A value of 1 indicates \"Brown\" cap color and 0 indicates \"Red\" cap color)<br>\n",
    "Tapering Shape (A value of 1 indicates \"Tapering Stalk Shape\" and 0 indicates \"Enlarging\" stalk shape)<br>\n",
    "Solitary (A value of 1 indicates \"Yes\" and 0 indicates \"No\")<br><br>\n",
    "y_train is whether the mushroom is edible<br>\n",
    "y = 1 indicates edible<br>\n",
    "y = 0 indicates poisonous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                                                    | Brown Cap | Tapering Stalk Shape | Solitary | Edible |\n",
    "|:--------------------------------------------------:|:---------:|:--------------------:|:--------:|:------:|\n",
    "| <img src=\"images/0.png\" alt=\"drawing\" width=\"50\"/> |     1     |           1          |     1    |    1   |\n",
    "| <img src=\"images/1.png\" alt=\"drawing\" width=\"50\"/> |     1     |           0          |     1    |    1   |\n",
    "| <img src=\"images/2.png\" alt=\"drawing\" width=\"50\"/> |     1     |           0          |     0    |    0   |\n",
    "| <img src=\"images/3.png\" alt=\"drawing\" width=\"50\"/> |     1     |           0          |     0    |    0   |\n",
    "| <img src=\"images/4.png\" alt=\"drawing\" width=\"50\"/> |     1     |           1          |     1    |    1   |\n",
    "| <img src=\"images/5.png\" alt=\"drawing\" width=\"50\"/> |     0     |           1          |     1    |    0   |\n",
    "| <img src=\"images/6.png\" alt=\"drawing\" width=\"50\"/> |     0     |           0          |     0    |    0   |\n",
    "| <img src=\"images/7.png\" alt=\"drawing\" width=\"50\"/> |     1     |           0          |     1    |    1   |\n",
    "| <img src=\"images/8.png\" alt=\"drawing\" width=\"50\"/> |     0     |           1          |     0    |    1   |\n",
    "| <img src=\"images/9.png\" alt=\"drawing\" width=\"50\"/> |     1     |           0          |     0    |    0   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[1,1,1],[1,0,1],[1,0,0],[1,0,0],[1,1,1],[0,1,1],[0,0,0],[1,0,1],[0,1,0],[1,0,0]])\n",
    "y_train = np.array([1,1,0,0,1,0,0,1,1,0])\n",
    "root_indices=np.array([0,1,2,3,4,5,6,7,8,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 3)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have three feature and ten data samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps for building a decision tree are as follows:<br>\n",
    "-Start with all examples at the root node,<br>\n",
    "-Calculate information gain for splitting on all possible features, and pick the one with the highest information gain,<br>\n",
    "-Split dataset according to the selected feature, and create left and right branches of the tree,<br>\n",
    "-Keep repeating splitting process until stopping criteria is met.<br>\n",
    "<br>\n",
    "In this lab, you'll implement the following functions, which will let you split a node into left and right branches using the feature with the\n",
    " highest information gain<br>\n",
    "-Calculate the entropy at a node<br>\n",
    "-Split the dataset at a node into left and right branches based on a given feature<br>\n",
    "-Calculate the information gain from splitting on a given feature<br>\n",
    "-Choose the feature that maximizes information gain<br>\n",
    "<br><br>\n",
    "For this lab, the stopping criteria we've chosen is setting a maximum depth of 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy is the minimum number of bits needed in order to classify an arbitrary\n",
    "example as yes or no. It is calculated as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll write a helper function called `compute_entropy` that computes the entropy (measure of impurity) at a node. \n",
    "- The function takes in a numpy array (`y`) that indicates whether the examples in that node are edible (`1`) or poisonous(`0`) \n",
    "- `Entropy` is the minimum number of bits needed in order to classify an arbitrary example as yes or no\n",
    "- Entropy(S)= expected number of bits needed to encode class (+ or -) of randomly drawn members of S. It depends from the distribution of the\n",
    "  random variable p.\n",
    "- `S` is a collection of training examples\n",
    "- `p_1` the proportion of positive examples in S\n",
    "- `1-p_1` the proportion of negative examples in S\n",
    " \n",
    "\n",
    "$$H(p_1) = -p_1 \\text{log}_2(p_1) - (1- p_1) \\text{log}_2(1- p_1)$$\n",
    "* Note \n",
    "    * The log is calculated with base $2$\n",
    "    * For implementation purposes, $0\\text{log}_2(0) = 0$. That is, if `p_1 = 0` or `p_1 = 1`, set the entropy to `0`\n",
    "    * Make sure to check that the data at a node is not empty (i.e. `len(y) != 0`). Return `0` if it is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Entropy is best explained in this video:\n",
    "https://www.youtube.com/watch?v=YtebGVx-Fxw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(y):\n",
    "    #y is ndarray of the target value \n",
    "    entropy = 0.\n",
    "    ones=0\n",
    "    zeros=0\n",
    "    len_y=len(y)\n",
    "    if len_y!=0:\n",
    "        ones=np.count_nonzero(y==1)\n",
    "        p1=ones/len_y\n",
    "        if p1 != 1:\n",
    "            if p1 !=0:\n",
    "                entropy=-p1*np.log2(p1)-(1-p1)*np.log2(1-p1)      \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Split dataset\n",
    "\n",
    "Next, we'll write a helper function called `split_dataset` that takes in the data at a node and a feature to split on and splits it into left \n",
    "and right branches.\n",
    "\n",
    "- The function takes in the training data, the list of indices of data points at that node, along with the feature to split on. \n",
    "- It splits the data and returns the subset of indices at the left and the right branch.\n",
    "- For example, say we're starting at the root node (so `node_indices = [0,1,2,3,4,5,6,7,8,9]`), and we chose to split on feature `0`, which is  \n",
    "whether or not the example has a brown cap. \n",
    "    - The output of the function is then, `left_indices = [0,1,2,3,4,7,9]` (data points with brown cap) and `right_indices = [5,6,8]` (data \n",
    "    points without a brown cap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                                                    | Brown Cap | Tapering Stalk Shape | Solitary | Edible |\n",
    "|:--------------------------------------------------:|:---------:|:--------------------:|:--------:|:------:|\n",
    "| <img src=\"images/0.png\" alt=\"drawing\" width=\"50\"/> |     1     |           1          |     1    |    1   |\n",
    "| <img src=\"images/1.png\" alt=\"drawing\" width=\"50\"/> |     1     |           0          |     1    |    1   |\n",
    "| <img src=\"images/2.png\" alt=\"drawing\" width=\"50\"/> |     1     |           0          |     0    |    0   |\n",
    "| <img src=\"images/3.png\" alt=\"drawing\" width=\"50\"/> |     1     |           0          |     0    |    0   |\n",
    "| <img src=\"images/4.png\" alt=\"drawing\" width=\"50\"/> |     1     |           1          |     1    |    1   |\n",
    "| <img src=\"images/5.png\" alt=\"drawing\" width=\"50\"/> |     0     |           1          |     1    |    0   |\n",
    "| <img src=\"images/6.png\" alt=\"drawing\" width=\"50\"/> |     0     |           0          |     0    |    0   |\n",
    "| <img src=\"images/7.png\" alt=\"drawing\" width=\"50\"/> |     1     |           0          |     1    |    1   |\n",
    "| <img src=\"images/8.png\" alt=\"drawing\" width=\"50\"/> |     0     |           1          |     0    |    1   |\n",
    "| <img src=\"images/9.png\" alt=\"drawing\" width=\"50\"/> |     1     |           0          |     0    |    0   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, node_indices, feature):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X (ndarray):             Data matrix of shape(n_samples, n_features)\n",
    "        node_indices (list):     List containing the active indices. I.e, the samples being considered at this step.\n",
    "        feature (int):           Index of feature to split on\n",
    "    \n",
    "    Returns:\n",
    "        left_indices (list):     Indices with feature value == 1\n",
    "        right_indices (list):    Indices with feature value == 0\n",
    "    \"\"\"\n",
    "    left_indices = []\n",
    "    right_indices = []\n",
    "    for i in range(len(node_indices)):\n",
    "        if X[node_indices[i]][feature]==1:\n",
    "            left_indices.append(node_indices[i])  \n",
    "        else:\n",
    "            right_indices.append(node_indices[i])       \n",
    "    return left_indices, right_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information gain\n",
    "`Information gain` is the expected reduction in entropy caused by `partitioning` the examples on an attribute.<br>\n",
    "The `higher` the information gain the more `effective` the attribute in `classifying` training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_information_gain(X, y, node_indices, feature):\n",
    "    \n",
    "    \"\"\"\n",
    "    Compute the information of splitting the node on a given feature\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray):            Data matrix of shape(n_samples, n_features)\n",
    "        y (array like):         list or ndarray with n_samples containing the target variable\n",
    "        node_indices (ndarray): List containing the active indices. I.e, the samples being considered in this step.\n",
    "   \n",
    "    Returns:\n",
    "        cost (float):        Cost computed\n",
    "    \n",
    "    \"\"\"    \n",
    "    # Split dataset\n",
    "    left_indices, right_indices = split_dataset(X, node_indices, feature)\n",
    "    \n",
    "    # Some useful variables\n",
    "    X_node, y_node = X[node_indices], y[node_indices]\n",
    "    X_left, y_left = X[left_indices], y[left_indices]\n",
    "    X_right, y_right = X[right_indices], y[right_indices]\n",
    "    information_gain = 0\n",
    "\n",
    "    h_p_node=compute_entropy(y_node)\n",
    "    h_p_left=compute_entropy(y_left)\n",
    "    h_p_right=compute_entropy(y_right)\n",
    "    w_left=len(y_left)/len(y_node)\n",
    "    w_right=len(y_right)/len(y_node)\n",
    "    \n",
    "    information_gain=h_p_node - (w_left*h_p_left + w_right*h_p_right)\n",
    "\n",
    "    return information_gain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To get best split we have to choose the feature with `high information gain.`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_split(X, y, node_indices):   \n",
    "    \"\"\"\n",
    "    Returns the optimal feature and threshold value\n",
    "    to split the node data \n",
    "    \n",
    "    Args:\n",
    "        X (ndarray):            Data matrix of shape(n_samples, n_features)\n",
    "        y (array like):         list or ndarray with n_samples containing the target variable\n",
    "        node_indices (ndarray): List containing the active indices. I.e, the samples being considered in this step.\n",
    "\n",
    "    Returns:\n",
    "        best_feature (int):     The index of the best feature to split\n",
    "    \"\"\"    \n",
    "    \n",
    "    # Some useful variables\n",
    "    num_features = X.shape[1]\n",
    "    en_temp=0.\n",
    "    # You need to return the following variables correctly\n",
    "    best_feature = -1\n",
    "    best_feature_indices=-1\n",
    "    for i in range(num_features):\n",
    "        en_temp=compute_information_gain(X, y, node_indices, feature=i)\n",
    "        if en_temp!=0 and en_temp > best_feature:\n",
    "            best_feature_indices=i \n",
    "    return best_feature_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree=[]\n",
    "def build_tree_recursive(X, y, node_indices, branch_name, max_depth, current_depth):\n",
    "    \"\"\"\n",
    "    Build a tree using the recursive algorithm that split the dataset into 2 subgroups at each node.\n",
    "    This function just prints the tree.\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray):            Data matrix of shape(n_samples, n_features)\n",
    "        y (array like):         list or ndarray with n_samples containing the target variable\n",
    "        node_indices (ndarray): List containing the active indices. I.e, the samples being considered in this step.\n",
    "        branch_name (string):   Name of the branch. ['Root', 'Left', 'Right']\n",
    "        max_depth (int):        Max depth of the resulting tree. \n",
    "        current_depth (int):    Current depth. Parameter used during recursive call.\n",
    "   \n",
    "    \"\"\" \n",
    "\n",
    "    # Maximum depth reached - stop splitting\n",
    "    if current_depth == max_depth:\n",
    "        formatting = \" \"*current_depth + \"-\"*current_depth\n",
    "        print(formatting, \"%s leaf node with indices\" % branch_name, node_indices)\n",
    "        return\n",
    "   \n",
    "    # Otherwise, get best split and split the data\n",
    "    # Get the best feature and threshold at this node\n",
    "    best_feature = get_best_split(X, y, node_indices) \n",
    "    \n",
    "    formatting = \"-\"*current_depth\n",
    "    print(\"%s Depth %d, %s: Split on feature: %d\" % (formatting, current_depth, branch_name, best_feature))\n",
    "    \n",
    "    # Split the dataset at the best feature\n",
    "    left_indices, right_indices = split_dataset(X, node_indices, best_feature)\n",
    "    tree.append((left_indices, right_indices, best_feature))\n",
    "    \n",
    "    # continue splitting the left and the right child. Increment current depth\n",
    "    build_tree_recursive(X, y, left_indices, \"Left\", max_depth, current_depth+1)\n",
    "    build_tree_recursive(X, y, right_indices, \"Right\", max_depth, current_depth+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Depth 0, Root: Split on feature: 2\n",
      "- Depth 1, Left: Split on feature: 1\n",
      "  -- Left leaf node with indices [0, 4, 5]\n",
      "  -- Right leaf node with indices [1, 7]\n",
      "- Depth 1, Right: Split on feature: 1\n",
      "  -- Left leaf node with indices [8]\n",
      "  -- Right leaf node with indices [2, 3, 6, 9]\n"
     ]
    }
   ],
   "source": [
    "build_tree_recursive(X_train, y_train, root_indices, \"Root\", max_depth=2, current_depth=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([0, 1, 4, 5, 7], [2, 3, 6, 8, 9], 2),\n",
       " ([0, 4, 5], [1, 7], 1),\n",
       " ([8], [2, 3, 6, 9], 1)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
